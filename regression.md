# Regression  
- y=w1x1
- 회귀의 목적 : 추세선, 최적의 w를 찾는 것 
- 집단은 평균값으로 다시 돌아오게 되어있다.  
- 가장 설명을 잘하는 직선을 찾자  
- 에러는 실제값-예측값  
- 통상적으로 에러의 제곱을 더 많이 쓴다 SE(Square Error) , MSE(Mean Square Error) 에러제곱의 합의 평균  
- 에러가 커지니까 (제곱하면) 조금만 줄여도 에러를 확확 줄일 수 있게 방향 설정 가능  
- Mean Square Error == Cost function 비용 함수(코스트는 줄여야함) == RSS(w) 잔차제곱합 
- Acc 스코어는 100을 향해 하고 MSE는 0을 향해가고  
- Cost function 의 최소값이 되는 하나의 w를 찾는게 목적  
- 제곱의 합이니까 제곱의 합으로 올라가게 됨.  어쨋든 RSS는 양쪽으로 제곱의 합만큼 올라감  
-  미분이 0인 지점을 찾아가야함  
-  얼마만큼 가야 0까지 갈 수 있는지 모름 가파른지 완만한지 모름  
-  0.9에서 얼마나 줄여야하는지 잘 모르는것  
-  미분을 했더니 +값이 나오거나 -값이 나오거나 하면 어디로 줄어얗ㄹ 지 방향은 알음  
-  -가 나오면 w를 늘ㄹ야한다 +가 나오면 w를 줄여야한다.  
-  w=0.3 ㄱㄴㄴ(w) 기울기 = - ,  W+?  
-  W=0.9    , 0.9+?
-  로지스틱에ㅓ learning rate가 있었잖아 이거 가중치를 얼마나 올리고 내릴지를 말하는 거엿음  
-  그래서 너무 작으면 ㅗ랜시간이 걸린다  
-  학습이 아예안되면 러닝레이트를 낮춰서 속도를 느리게 하고..  
-  이런건 하이퍼파라미터임  
-  경사상승이 되서는 안된다  
-  어떻게 이걸 자동으로 검색ㅎ게 할가  
-  
- RSS(w1)을 미분하면 1/N 시그마(-2yixi + 2wx(xi)^2)
- == -2/N 시그마 (yi-xi-w(xi)^2)  
- ^y 는 예측값임  
- 이 식을 자동화르 시켜야함  
- 어쩃든 앞에 마이너스까지 포함한 결과가 +면 줄어야하고(뺴야하고ㅒ) 밥대면 반대  
- ar(w)/aw를 왜 뺴냐면 기울기가 작앚ㅅ다  ==w를 찾는 과정  
- 점점더 작은 값을 빼주는 것임  
- w를 찾는 기준이 경사하강법 임  
- 사실 1차회귀할때는 경사하강법 안써도된다  연립방정식 써도 w를 구할 수 있따  
- bias는 절편..?  
- 변수가 두개이상인데미분을 어케 하나요> ==> 편미분  
- 
- RSS(W0,W1) = 1/N 시그마(아이는 일부터 엔까지) (yi-^yi)^2  
-  = 1/ㅜ
-  앞시점의 w1에서 w1미분한 값을 빼준다 
-  양수 음수 알아서 ㅇㅇ  
-  계산량이 많은거지 계산이 복잡한것은 아님   
-  w0 w1 둘다 동시에 값이 업데이트??  
-  약속한 값에 도달하면 그 시점의 w0 w1 값을 보면 최적의 w를 구한것이다  
- 절편도 기울기도 바뀐ㄷ.  

출력값은 연속값  
변수(피처)가 n개면 종속변수 1개 빼고 나머지 xtrain에 들어가는 변수를 넣으면 12개의 x값이 잇고 그거에 대응하는 w값이 잇는데
뭐가 하나 더 생겨서 13개 편미분을 함  
w0+x1w1+x2w2+ ... xnwn..  

회귀는 사이킷런에 아주쉽게 구현 !!  
linear regression 선형 회귀  
fit_ntercept 는 당연히 true 절편은 무조건 계산해얗ㅁ  
굳이 feature scalㅣㅑㅜㅎdmf dks안해도 normalize = True 해주면 정규호홰줌  

사이킷런 선형회귀를 이용한 보스턴 주택 가격 예측  
회귀 평가 지표 MAE는 잘 안씀  
주로 R^2 MSE RMSE 누가 더 0에 에러가 가깝게 예측을 햇느냐가 평가 지표가 된다.  



